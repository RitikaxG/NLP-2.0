{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8274f132",
   "metadata": {},
   "source": [
    "### Implementing Word2Vec from Scratch\n",
    "* Continuous Bag of Words(CBOW) : Take the neighboring words, take two words before and two words after, predict middle word through these context words.\n",
    "* Skip Gram : Take middle word and present context word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12849b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ritikagupta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0bf9b",
   "metadata": {},
   "source": [
    "#### 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a63c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = open('big.txt','r')\n",
    "text_data = fd.read()\n",
    "fd.close()\n",
    "\n",
    "# From the whole corpus take only the 1st 100,000 words\n",
    "text_data = text_data[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003273a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data) # No of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824bdf7c",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing | Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e2e4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_chars = '!@#$%^&*()_+=-|\\()[]{}:;<>?\\n'\n",
    "\n",
    "for char in special_chars:\n",
    "    text_data = text_data.replace(char, ' ')\n",
    "\n",
    "text_data = text_data.replace(\"'\", ' ')\n",
    "text_data = text_data.replace(\"  \", ' ')\n",
    "text_data = text_data.replace(\"  \", ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a286987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of The Adventures of Sherlock Holmes by Sir Arthur Conan Doyle 15 in our'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f715f",
   "metadata": {},
   "source": [
    "#### 3. Creating Word Index and Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16803cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting words in a sentence\n",
    "words = word_tokenize(text_data)\n",
    "# Removing null\n",
    "words = [word for word in words if len(word)!=0]\n",
    "# Every unique word in the corpus is assigned with a number\n",
    "word_index = {word:i for i, word in enumerate(set(words))}\n",
    "# Get word from number assigned to it\n",
    "index_word = {word_index[word] : word for word in word_index}\n",
    "\n",
    "sents = [word_tokenize(sent) for sent in sent_tokenize(text_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3999132c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of The Adventures of Sherlock Holmes by Sir Arthur Conan Doyle 15 in our series by Sir Arthur Conan Doyle Copyright laws are changing all over the world .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sents[0])\n",
    "# The Project Gutenberg EBook of \n",
    "# Gutenberg : Middle word\n",
    "# Project Gutenberg EBook of The : Next pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770501ba",
   "metadata": {},
   "source": [
    "#### 4. Creating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a4d7912",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels   = []\n",
    "window_size = 2 # Gutenberg : Output || The, Project,Ebook, of : Input\n",
    "for sent in sents:\n",
    "    # Total no of pairs\n",
    "    # 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'The', 'Adventures', 'of', 'Sherlock' : Total Pairs: 7 for window size =1 , No of words : 9\n",
    "    # for window_size = 2, Pairs = 5, n = 9 || 9 -(2*2) = 5\n",
    "    # for window_size = 3, Pairs = 3, n (no of words in sentence) = 9 || 9-(3*2) = 3 {Length of sentence - window_size*2}\n",
    "    for i in range(len(sent) - window_size*2):\n",
    "        # Input pair\n",
    "        features.append(sent[i:i+window_size] + sent[i+window_size+1 : i+window_size*2+1])\n",
    "        # Output\n",
    "        labels.append(sent[i+window_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "219dd92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'EBook', 'of'] Gutenberg\n",
      "['Project', 'Gutenberg', 'of', 'The'] EBook\n",
      "['Gutenberg', 'EBook', 'The', 'Adventures'] of\n",
      "['EBook', 'of', 'Adventures', 'of'] The\n",
      "['of', 'The', 'of', 'Sherlock'] Adventures\n",
      "['The', 'Adventures', 'Sherlock', 'Holmes'] of\n",
      "['Adventures', 'of', 'Holmes', 'by'] Sherlock\n",
      "['of', 'Sherlock', 'by', 'Sir'] Holmes\n",
      "['Sherlock', 'Holmes', 'Sir', 'Arthur'] by\n",
      "['Holmes', 'by', 'Arthur', 'Conan'] Sir\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(features[i],labels[i])\n",
    "# Convert the features and labels into one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18a7ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for feature in features:\n",
    "    # Creating a vector of all values as zero\n",
    "    enc = np.zeros(len(word_index))\n",
    "    for word in feature:\n",
    "        # Find word index of context word and replace 0 with 1 at those index\n",
    "        enc[word_index[word]] = 1\n",
    "    X_train.append(enc)\n",
    "    \n",
    "for label in labels:\n",
    "    enc = np.zeros(len(word_index))\n",
    "    enc[word_index[label]] = 1\n",
    "    y_train.append(enc)\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bef47b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17652, 3457)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # 3457 : no of words  # 17652 : no of pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a766ed7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17652, 3457)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a21a3fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b72bc194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa30fd5",
   "metadata": {},
   "source": [
    "#### 5. Building neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f9ad944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 6.4912 - accuracy: 0.0719\n",
      "Epoch 2/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 5.7746 - accuracy: 0.1006\n",
      "Epoch 3/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 5.4366 - accuracy: 0.1398\n",
      "Epoch 4/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 5.1318 - accuracy: 0.1702\n",
      "Epoch 5/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 4.8305 - accuracy: 0.2022\n",
      "Epoch 6/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 4.5094 - accuracy: 0.2389\n",
      "Epoch 7/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 4.1634 - accuracy: 0.2810\n",
      "Epoch 8/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 3.7912 - accuracy: 0.3223\n",
      "Epoch 9/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 3.3962 - accuracy: 0.3671\n",
      "Epoch 10/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 2.9859 - accuracy: 0.4113\n",
      "Epoch 11/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 2.5628 - accuracy: 0.4636\n",
      "Epoch 12/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 2.1462 - accuracy: 0.5248\n",
      "Epoch 13/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 1.7591 - accuracy: 0.6083\n",
      "Epoch 14/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 1.4330 - accuracy: 0.7001\n",
      "Epoch 15/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 1.1872 - accuracy: 0.7559\n",
      "Epoch 16/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 1.0071 - accuracy: 0.7959\n",
      "Epoch 17/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.8715 - accuracy: 0.8213\n",
      "Epoch 18/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.7652 - accuracy: 0.8456\n",
      "Epoch 19/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.6776 - accuracy: 0.8605\n",
      "Epoch 20/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.6030 - accuracy: 0.8753\n",
      "Epoch 21/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.5401 - accuracy: 0.8910\n",
      "Epoch 22/50\n",
      "552/552 [==============================] - 3s 5ms/step - loss: 0.4862 - accuracy: 0.9001\n",
      "Epoch 23/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.4390 - accuracy: 0.9104\n",
      "Epoch 24/50\n",
      "552/552 [==============================] - 3s 5ms/step - loss: 0.3983 - accuracy: 0.9190\n",
      "Epoch 25/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.3605 - accuracy: 0.9280\n",
      "Epoch 26/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.3294 - accuracy: 0.9330\n",
      "Epoch 27/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.3011 - accuracy: 0.9398\n",
      "Epoch 28/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.2754 - accuracy: 0.9431\n",
      "Epoch 29/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.2525 - accuracy: 0.9494\n",
      "Epoch 30/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.2326 - accuracy: 0.9524\n",
      "Epoch 31/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.2144 - accuracy: 0.9559\n",
      "Epoch 32/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.1967 - accuracy: 0.9593\n",
      "Epoch 33/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.1830 - accuracy: 0.9618\n",
      "Epoch 34/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.1688 - accuracy: 0.9652\n",
      "Epoch 35/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.1574 - accuracy: 0.9679\n",
      "Epoch 36/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.1453 - accuracy: 0.9703\n",
      "Epoch 37/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.1356 - accuracy: 0.9709\n",
      "Epoch 38/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.1267 - accuracy: 0.9728\n",
      "Epoch 39/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.1184 - accuracy: 0.9741\n",
      "Epoch 40/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.1112 - accuracy: 0.9758\n",
      "Epoch 41/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.1047 - accuracy: 0.9766\n",
      "Epoch 42/50\n",
      "552/552 [==============================] - 2s 3ms/step - loss: 0.0976 - accuracy: 0.9777\n",
      "Epoch 43/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.0918 - accuracy: 0.9792\n",
      "Epoch 44/50\n",
      "552/552 [==============================] - 3s 6ms/step - loss: 0.0878 - accuracy: 0.9793\n",
      "Epoch 45/50\n",
      "552/552 [==============================] - 3s 6ms/step - loss: 0.0825 - accuracy: 0.9802\n",
      "Epoch 46/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.0778 - accuracy: 0.9818\n",
      "Epoch 47/50\n",
      "552/552 [==============================] - 3s 5ms/step - loss: 0.0746 - accuracy: 0.9810\n",
      "Epoch 48/50\n",
      "552/552 [==============================] - 3s 6ms/step - loss: 0.0707 - accuracy: 0.9822\n",
      "Epoch 49/50\n",
      "552/552 [==============================] - 3s 5ms/step - loss: 0.0677 - accuracy: 0.9822\n",
      "Epoch 50/50\n",
      "552/552 [==============================] - 2s 4ms/step - loss: 0.0651 - accuracy: 0.9829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x290a7e390>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No of neurons in input, hidden, output layer : 3425 , 100, 3425\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100,input_dim = len(word_index),activation = 'relu'))\n",
    "model.add(Dense(len(word_index),activation= 'softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train,epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7adb82",
   "metadata": {},
   "source": [
    "#### Extract word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3965bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every word we are having a vector of size 100\n",
    "word_embedding = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4aaafd",
   "metadata": {},
   "source": [
    "#### Finding similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fa51bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3457, 100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cab7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word):    \n",
    "    target_word = word_embedding[word_index[word]]\n",
    "    # Save distance of all words from the target word\n",
    "    distances = np.dot(word_embedding, target_word)\n",
    "    # Returns the index of 10 most similar words\n",
    "    most_similar = np.argsort(distances)[::-1][:5]\n",
    "\n",
    "    print(\"Most similar word of  \",word,\"is :\",[index_word[i] for i in most_similar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7106751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar word of   EBook is : ['EBook', 'breathing', 'wrist', 'Mortimer', 'fish']\n"
     ]
    }
   ],
   "source": [
    "most_similar('EBook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27309909",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7f76e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
