{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb572af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus =  '''\n",
    "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and understanding. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process.[1] LLMs are artificial neural networks, the largest and most capable of which are built with a transformer-based architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).[2][3][4]\n",
    "LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.[5] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[6] They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[7]\n",
    "Some notable LLMs are OpenAI's GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google's PaLM and Gemini (the latter of which is currently used in the chatbot of the same name), Meta's LLaMA family of open-source models, and Anthropic's Claude models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcc0b6",
   "metadata": {},
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c04acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for word in word_tokenize(corpus):\n",
    "    if(word.lower() not in stopwords.words('english')) and (len(word)>=2):\n",
    "        words.append(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40f343",
   "metadata": {},
   "source": [
    "### Creating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b46b25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(words))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e1632",
   "metadata": {},
   "source": [
    "### Creating Encoders and Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9d7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "word_to_num = {}\n",
    "num_to_word = {}\n",
    "for word in vocab:\n",
    "    word_to_num[word] = num\n",
    "    num_to_word[num] = word\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a2623ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_num['gemini']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a748c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recurrent'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_to_word[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5aa3c1",
   "metadata": {},
   "source": [
    "### Encoding Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "748f0d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large language model LLM language model notable ability achieve general-purpose language generation understanding \n",
      "[31, 25, 32, 41, 25, 32, 23, 87, 56, 1, 25, 27, 94]\n",
      "LLMs acquire abilities learning statistical relationships text documents computationally intensive self-supervised semi-supervised training process \n",
      "[74, 83, 38, 85, 9, 24, 30, 105, 84, 103, 39, 15, 92, 48]\n",
      "LLMs artificial neural networks largest capable built transformer-based architecture \n",
      "[74, 58, 71, 13, 20, 54, 12, 16, 98]\n",
      "recent implementations based architectures recurrent neural network variants Mamba state space model \n",
      "[69, 57, 21, 108, 4, 71, 36, 60, 63, 26, 80, 32]\n",
      "LLMs used text generation form generative AI taking input text repeatedly predicting next token word \n",
      "[74, 102, 30, 27, 82, 97, 2, 49, 66, 30, 35, 61, 76, 62, 65]\n",
      "2020 fine tuning way model could adapted able accomplish specific tasks \n",
      "[96, 34, 107, 99, 32, 33, 28, 64, 68, 45, 18]\n",
      "Larger sized models GPT-3 however prompt-engineered achieve similar results \n",
      "[93, 59, 79, 17, 90, 5, 56, 3, 8]\n",
      "thought acquire knowledge syntax semantics `` ontology '' inherent human language corpora also inaccuracies biases present corpora \n",
      "[106, 83, 44, 73, 55, 10, 53, 89, 50, 19, 25, 100, 6, 52, 91, 51, 100]\n",
      "notable LLMs OpenAI 's GPT series models e.g. GPT-3.5 GPT-4 used ChatGPT Microsoft Copilot Google 's PaLM Gemini latter currently used chatbot name Meta 's LLaMA family open-source models Anthropic 's Claude models \n",
      "[23, 74, 72, 42, 81, 43, 79, 22, 101, 86, 102, 75, 29, 11, 104, 42, 95, 46, 67, 37, 102, 40, 77, 14, 42, 47, 88, 70, 79, 78, 42, 7, 79]\n"
     ]
    }
   ],
   "source": [
    "# Taking each sentence and passing it through word tokenizer\n",
    "data = []\n",
    "for sent in sent_tokenize(corpus):\n",
    "    temp = []\n",
    "    for word in word_tokenize(sent):\n",
    "        if(word.lower() not in stopwords.words('english')) and(len(word) >= 2):\n",
    "            print(word,end=' ')\n",
    "            temp.append(word_to_num[word.lower()])\n",
    "    print()\n",
    "    print(temp)\n",
    "    \n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e0403",
   "metadata": {},
   "source": [
    "### Decoding Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fbd7d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large language model llm language model notable ability achieve general-purpose language generation understanding \n",
      "llms acquire abilities learning statistical relationships text documents computationally intensive self-supervised semi-supervised training process \n",
      "llms artificial neural networks largest capable built transformer-based architecture \n",
      "recent implementations based architectures recurrent neural network variants mamba state space model \n",
      "llms used text generation form generative ai taking input text repeatedly predicting next token word \n",
      "2020 fine tuning way model could adapted able accomplish specific tasks \n",
      "larger sized models gpt-3 however prompt-engineered achieve similar results \n",
      "thought acquire knowledge syntax semantics `` ontology '' inherent human language corpora also inaccuracies biases present corpora \n",
      "notable llms openai 's gpt series models e.g. gpt-3.5 gpt-4 used chatgpt microsoft copilot google 's palm gemini latter currently used chatbot name meta 's llama family open-source models anthropic 's claude models \n"
     ]
    }
   ],
   "source": [
    "for sent in data:\n",
    "    for word in sent:\n",
    "        print(num_to_word[word],end= ' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf112ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
