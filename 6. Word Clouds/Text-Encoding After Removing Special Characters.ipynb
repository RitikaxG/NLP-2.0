{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520765dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus =  '''\n",
    "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and understanding. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process.[1] LLMs are artificial neural networks, the largest and most capable of which are built with a transformer-based architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).[2][3][4]\n",
    "LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.[5] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[6] They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[7]\n",
    "Some notable LLMs are OpenAI's GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google's PaLM and Gemini (the latter of which is currently used in the chatbot of the same name), Meta's LLaMA family of open-source models, and Anthropic's Claude models.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa0a07c",
   "metadata": {},
   "source": [
    "### Text Preprocessing by Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1374f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for word in word_tokenize(corpus):\n",
    "    if(len(word) == 1):\n",
    "        if(ord(word)>=97 and ord(word)<=122) or (ord(word)>=65 and ord(word)<=90):\n",
    "            words.append(word.lower())\n",
    "    else:\n",
    "        words.append(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e459f25e",
   "metadata": {},
   "source": [
    "#### Creating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce4b688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15129df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e465d",
   "metadata": {},
   "source": [
    "### Creating Encoders and Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c5598b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "word_to_num = {}\n",
    "num_to_word = {}\n",
    "for word in vocab:\n",
    "    word_to_num[word] = num\n",
    "    num_to_word[num] = word\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634170bf",
   "metadata": {},
   "source": [
    "### Encoding Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291a7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking each sentence and passing it through word tokenizer\n",
    "data = []\n",
    "for sent in sent_tokenize(corpus):\n",
    "    temp = []\n",
    "    for word in word_tokenize(sent):\n",
    "        if(len(word) == 1):\n",
    "            if(ord(word)>=97 and ord(word)<=122) or (ord(word)>=65 and ord(word)<=90):\n",
    "                temp.append(word_to_num[word.lower()])\n",
    "        else:\n",
    "            temp.append(word_to_num[word.lower()])\n",
    "            \n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb25b7",
   "metadata": {},
   "source": [
    "### Decoding Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "096e0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a large language model llm is a language model notable for its ability to achieve general-purpose language generation and understanding \n",
      "llms acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process \n",
      "llms are artificial neural networks the largest and most capable of which are built with a transformer-based architecture \n",
      "some recent implementations are based on other architectures such as recurrent neural network variants and mamba a state space model \n",
      "llms can be used for text generation a form of generative ai by taking an input text and repeatedly predicting the next token or word \n",
      "up to 2020 fine tuning was the only way a model could be adapted to be able to accomplish specific tasks \n",
      "larger sized models such as gpt-3 however can be prompt-engineered to achieve similar results \n",
      "they are thought to acquire knowledge about syntax semantics and `` ontology '' inherent in human language corpora but also inaccuracies and biases present in the corpora \n",
      "some notable llms are openai 's gpt series of models e.g. gpt-3.5 and gpt-4 used in chatgpt and microsoft copilot google 's palm and gemini the latter of which is currently used in the chatbot of the same name meta 's llama family of open-source models and anthropic 's claude models \n"
     ]
    }
   ],
   "source": [
    "for sent in data:\n",
    "    for word in sent:\n",
    "        print(num_to_word[word],end= ' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ca3f34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 102 139 125 47 127 51 139 125 39 58 33 83 119 57 48 139 9 68 88 \n",
      "62 137 30 54 85 93 61 79 6 135 55 124 51 74 76 118 68 26 136 104 \n",
      "62 14 53 140 77 31 25 68 82 80 3 126 14 131 15 51 87 40 \n",
      "111 138 72 14 49 94 64 8 133 1 32 140 20 56 68 129 51 99 2 125 \n",
      "62 67 109 114 58 135 9 51 121 3 110 134 85 95 116 12 135 68 96 7 31 115 41 89 71 \n",
      "90 119 73 11 28 117 31 105 66 51 125 34 109 128 119 109 92 119 86 23 37 \n",
      "4 21 101 133 1 97 22 67 109 112 119 57 78 45 \n",
      "63 14 122 119 137 59 10 5 107 68 60 27 46 91 24 69 139 65 123 141 50 68 132 70 24 31 65 \n",
      "111 39 62 14 38 29 84 19 3 101 13 44 68 18 114 24 100 68 108 120 42 29 130 68 52 31 106 3 126 127 43 114 24 31 98 3 31 75 81 113 29 103 36 3 16 101 68 35 29 17 101 \n"
     ]
    }
   ],
   "source": [
    "for sent in data:\n",
    "    for word in sent:\n",
    "        print(word,end = ' ')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
